{
  "name": "GPT-750M-1024-ChatSFT",
  "description": "750M chat fine-tuning. Lower LR, loss masking enabled.",
  "batch_size": 8,
  "block_size": 1024,
  "vocab_size": 50304,
  "n_embd": 1280,
  "n_head": 20,
  "n_layer": 32,
  "dropout": 0.05,
  "bias": false,
  "tie_weights": true,
  "pos_encoding": "rope",
  "mlp_type": "swiglu",
  "norm_type": "rmsnorm",
  "rope_theta": 10000.0,
  "learning_rate": 3e-5,
  "use_loss_mask": true
}

