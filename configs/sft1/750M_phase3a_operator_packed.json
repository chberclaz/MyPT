{
  "name": "GPT-750M-Phase3a-Operator-Packed",
  "description": "Operator learning with packed sequences (100k episodes, 37% mask ratio, 57x efficiency). Layer freezing to preserve pre-trained induction heads and prevent memorization.",
  
  "batch_size": 16,
  "block_size": 1024,
  "vocab_size": 50304,
  "n_embd": 1280,
  "n_head": 20,
  "n_layer": 32,
  "dropout": 0.05,
  "bias": false,
  "tie_weights": true,
  
  "learning_rate": 3e-5,
  "max_iters": 1200,
  "eval_interval": 100,
  "warmup_iters": 120,
  "use_amp": true,
  "amp_dtype": "bf16",
  
  "use_loss_mask": true,
  
  "freeze_layers": 28,
  "freeze_embeddings": false,
  
  "batch_sampling_mode": "epoch",
  "pad_token_id": null,
  "episode_min_tokens": 10,
  "epoch_seed": 7778,
  "epoch_shuffle": true,
  "epoch_drop_last": true,
  
  "weight_decay": 0.05,
  
  "eval_sets": null,
  "eval_seed": 1337,
  "log_file": "logs/train/phase3a_operator_packed.jsonl"
}
