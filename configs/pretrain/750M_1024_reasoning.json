{
  "name": "GPT-750M-1024-Reasoning",
  "description": "750M optimized for reasoning: deeper (32 layers) + narrower (1280 embd). Better multi-step logic, tool-calling decisions.",
  "batch_size": 12,
  "block_size": 1024,
  "vocab_size": 50304,
  "n_embd": 1280,
  "n_head": 20,
  "n_layer": 32,
  "dropout": 0.1,
  "bias": false,

  "_comment_training": "Training hyperparameters for 1.5B dataset, 2.5 epochs",
  "learning_rate": 1.5e-4,
  "max_iters": 305000,
  "eval_interval": 1000,
  "warmup_iters": 0.05
}
