{
  "name": "GPT-750M-1024-Domain-Adaptation",
  "description": "Domain adaptation config for 750M reasoning model. Lower LR + shorter warmup to preserve base knowledge while learning domains (IT security, protocols, Bash, Python/JS/Java docs, Swiss Law DE/EN). 161M tokens, 5 epochs (~805M token views). Use with --init_from_model.",

  "batch_size": 12,
  "block_size": 1024,
  "vocab_size": 50304,
  "n_embd": 1280,
  "n_head": 20,
  "n_layer": 32,
  "dropout": 0.1,
  "bias": false,

  "learning_rate": 5e-5,
  "max_iters": 65500,
  "eval_interval": 2000,
  "warmup_iters": 1300,
  "grad_clip": 1.0,
  "weight_decay": 0.1,
  "use_amp": true,
  "amp_dtype": "bf16",

  "_comment_eval": "Optional: Dual eval sets for domain vs general evaluation",
  "eval_sets": {
    "domain": "data/domain_161M_corpus_tokenized",
    "general": "data/phase1_base_tokenized"
  },
  "eval_seed": 1337,
  "log_file": "logs/train/domain_adapt_eval.jsonl"
}
