"""
MyPT Special Tokens
====================
19 special tokens that define the structural protocol for all MyPT interactions.
These are registered as additional vocabulary entries (IDs 50257+) beyond the
base GPT-2 tokenizer. Each token is learned during SFT, initialized from
meaningful base tokens via init_special_embeddings.py.

Token pairs and their purpose:
----------------------------------------------------------------------

CONVERSATION STRUCTURE (6 tokens)
  <myPT_system>     ... </myPT_system>
      System prompt. Sets the assistant's identity, rules, and constraints.
      Injected once at the start of each conversation. Loss-masked during
      SFT (model never trains to generate system prompts).

  <myPT_user>       ... </myPT_user>
      User message. Wraps each user turn in a conversation.
      Loss-masked during SFT (model never trains to generate user text).

  <myPT_assistant>  ... </myPT_assistant>
      Assistant response. Wraps each assistant turn. This is the only
      block where loss is computed during SFT -- the model learns to
      generate everything inside these tags.

RAG CONTEXT (4 tokens)
  <myPT_user_context>      ... </myPT_user_context>
      Retrieved passages injected alongside the user message. Contains
      chunks from the RAG index that are relevant to the user's query.
      Placed inside or before the user block. Loss-masked.

  <myPT_assistant_context> ... </myPT_assistant_context>
      Context injected for the assistant's use (e.g., workspace metadata,
      document summaries). Distinct from user_context because the source
      is the system, not the retrieval pipeline. Loss-masked.

TOOL USE (4 tokens)
  <myPT_toolcall>   ... </myPT_toolcall>
      Tool invocation. Generated BY the assistant inside its response.
      Contains a JSON payload: {"name": "tool.name", "param": "value"}.
      The agent controller parses this, executes the tool, and injects
      the result. Model IS trained to generate these.

  <myPT_toolresult> ... </myPT_toolresult>
      Tool execution result. Injected BY the system after a toolcall is
      executed. Contains the tool's return value (JSON or text).
      Loss-masked (model doesn't generate these, the system does).

REASONING (2 tokens)
  <myPT_think>      ... </myPT_think>
      Chain-of-thought scratchpad. Generated BY the assistant at the
      start of its response to reason before answering. Content is
      stripped from the response shown to the user during inference.
      Enables a 700M model to break complex tasks into steps internally.
      Model IS trained to generate these.

ATTRIBUTION (2 tokens)
  <myPT_cite>       ... </myPT_cite>
      Source citation. Generated BY the assistant to attribute answers
      to specific documents or chunks. Content is programmatically
      parseable for verification (e.g., "workspace/doc.pdf, chunk 3").
      Model IS trained to generate these.

CONTROL (1 token)
  <myPT_eot>
      End of turn. Marks the boundary between conversation turns.
      Used as the stop token during generation -- when the model emits
      this token, generation halts.

----------------------------------------------------------------------
Total: 19 tokens (IDs 50257-50275)
Vocab size: 50304 (base 50257 + special tokens, padded to 64)
"""

SPECIAL_TOKEN_STRINGS = {
    # Structure: conversation roles
    "myPT_system_open": "<myPT_system>",
    "myPT_system_close": "</myPT_system>",
    "myPT_user_open": "<myPT_user>",
    "myPT_user_close": "</myPT_user>",
    "myPT_assistant_open": "<myPT_assistant>",
    "myPT_assistant_close": "</myPT_assistant>",
    # RAG context: injected retrieval passages
    "myPT_user_context_open": "<myPT_user_context>",
    "myPT_user_context_close": "</myPT_user_context>",
    "myPT_assistant_context_open": "<myPT_assistant_context>",
    "myPT_assistant_context_close": "</myPT_assistant_context>",
    # Tool use: agentic tool calling and results
    "myPT_toolcall_open": "<myPT_toolcall>",
    "myPT_toolcall_close": "</myPT_toolcall>",
    "myPT_toolresult_open": "<myPT_toolresult>",
    "myPT_toolresult_close": "</myPT_toolresult>",
    # Reasoning: chain-of-thought scratchpad (hidden from user during inference)
    "myPT_think_open": "<myPT_think>",
    "myPT_think_close": "</myPT_think>",
    # Attribution: source citations (parseable for verification)
    "myPT_cite_open": "<myPT_cite>",
    "myPT_cite_close": "</myPT_cite>",
    # Control: end of turn separator
    "myPT_eot": "<myPT_eot>",
}

# ---------------------------------------------------------------------------
# Canonical ID resolution
# ---------------------------------------------------------------------------
# IDs are assigned sequentially starting at BASE_VOCAB_SIZE, following the
# insertion order of SPECIAL_TOKEN_STRINGS above.  This is the single source
# of truth -- every script that needs a token ID MUST use get_special_token_ids()
# instead of hardcoding numeric constants.  Adding or reordering tokens in the
# dict will automatically propagate to all consumers.
# ---------------------------------------------------------------------------

BASE_VOCAB_SIZE = 50257  # GPT-2 base vocabulary size

def get_special_token_ids() -> dict:
    """Return the canonical name -> token-ID mapping.

    Computed from the dict-insertion order of SPECIAL_TOKEN_STRINGS so it
    stays in sync automatically when tokens are added, removed, or reordered.

    Example::

        ids = get_special_token_ids()
        ASSISTANT_OPEN_ID = ids["myPT_assistant_open"]   # always correct
    """
    return {name: BASE_VOCAB_SIZE + i
            for i, name in enumerate(SPECIAL_TOKEN_STRINGS)}