Phase 3a0 â€” Chat protocol lock-in (hard syntax)

Goal: The model always emits the correct role/tags and never leaks other roles.

Strict <myPT_user> â€¦ <myPT_assistant> â€¦ structure

No tools yet, no JSON requirements yet

Include a small % malformed prompts only if you want robustness, but keep it low

Pass criteria: Near-zero tag violations on an eval set of adversarial prompts.

Phase 3a1 â€” Ultra-short compliance (your current run)

Goal: â€œObey instruction â†’ respond minimally â†’ stay in assistant tagâ€.

One-word answers, â€œYES/NOâ€, single token labels, etc.

Trains response length control and stop discipline

Great place to teach: â€œIf unknown â†’ say I don't know.â€ (or your preferred canonical)

Pass criteria: Response length + format is stable even under prompt variation.

Phase 3a2 â€” Minimal Q&A (short, 1 sentence answers)

Goal: â€œNormal user question â†’ short correct answerâ€ (no verbosity).

Small questions, 1 sentence answers (your suggestion is exactly right)

Mix styles: statements, imperatives, incomplete prompts, typos, â€œgive me X in 5 wordsâ€, etc.

Include â€œrefuse / safe-completeâ€ behaviors if you want them later

Pass criteria: Doesnâ€™t ramble; obeys constraints like â€œone sentenceâ€, â€œmax 10 wordsâ€.

Phase 3a3 â€” Controlled verbosity ladder (short â†” medium)

Goal: Teach the model to choose the right length on command.

Same intent asked with different constraints:

â€œAnswer in 1 sentenceâ€

â€œAnswer in 3 bullet pointsâ€

â€œExplain like Iâ€™m in a hurryâ€

â€œGive steps 1â€“3 onlyâ€

This is where many models break if not trained: they default to one â€œhouse styleâ€.

Pass criteria: Constraint-following generalizes (new topics, same instruction types).

Phase 3a4 â€” Text-only task following (multi-step, no tools)

Goal: Get the â€œagent coreâ€ without tools: plans, transforms, structured outputs.

Examples:

Extract â†’ normalize â†’ reformat

Classify + give reason (short)

Write a checklist from a paragraph

Summarize with specific schema (but still assistant text)

This phase is the bridge to tool calling because it trains:

step discipline

schema discipline

â€œdo X, then Y, then stopâ€

Pass criteria: Executes multi-step instructions without inventing extra steps or formats.

Phase 3a5 â€” Structured outputs (schemas, but still no tool execution)

Goal: Output that is machine-parseable consistently.

You can pick one or more:

Strict JSON (with escaping rules)

YAML

â€œtagged blocksâ€ (your MyPT tags + internal sections)

Function-call-like stubs (but not real tool calls yet)

This is where you prepare the exact shape you want for 3b:

arguments formatting

deterministic keys ordering (if you care)

no trailing commentary

Pass criteria: 99%+ parse success on held-out prompts.

Phase 3a6 â€” Tool-readiness simulation (fake tools, text-only)

Goal: Teach: decide when a tool is needed and compose the call, without actually calling.

Pattern:

User asks something that clearly requires a tool (search DB / calculate / retrieve file)

Assistant responds with a â€œtool request blockâ€ in your intended format

Then you include the â€œtool resultâ€ (as if <myPT_tool> â€¦) and train the assistant to finish.

Even if you donâ€™t enable <myPT_tool> until 3b, you can still simulate it with placeholder tags or a â€œTOOL_REQUEST:â€ block.

Pass criteria: Correct â€œcall vs no-callâ€ decision + correct argument filling + clean finalization.

Recommended â€œminimal setâ€ if you want to move fast

If speed matters and you want to reach 3b ASAP, you can compress to:

3a0 protocol

3a1 ultra-short control

3a2 minimal Q&A

3a4 multi-step text tasks

3a5 schema outputs
â†’ then Phase 3b real tool calls

(3a3 is very valuable, but it can be partially covered by making 3a2/3a4 include explicit length constraints.)

One critical warning (based on your earlier pain)

Avoid mixing too many styles inside one run early on (that was the â€œgatewayâ€ feeling you described). Keep each sub-phase â€œsingle-purposeâ€, then merge later with small weights.

A typical merge strategy:

current focus dataset: 70â€“85%

replay of previous 3a phases: 10â€“25%

tiny general replay (optional): 0â€“10% (only if you see language degradation)

What Phase 3b needs from 3a to be easy

By the time you start toolcalls, the model should already:

always produce valid MyPT chat tags

obey output constraints (short, schema, stop)

handle multi-step instructions

emit strict structured blocks reliably

decide â€œtool needed?â€ vs â€œanswer directly?â€

If those are true, 3b becomes mostly: â€œnew output head format + supervised examples of tool selection.â€

If you want, paste your planned 3b tool-call envelope (how you want <myPT_tool> to look, how args are encoded), and Iâ€™ll map 3a5/3a6 exactly onto that so your transition is frictionless.

How to train summarizing (Phase 3a)
1) Put summarization in the right place

Summarization is Phase 3a4/3a5-ish: itâ€™s task-following + controlled output, not just â€œshort answersâ€.

If you try to teach summarization during 3a1/3a2, youâ€™ll get:

over-short â€œsummariesâ€ (too lossy)

bad constraint following (â€œ1 sentenceâ€ but rambles)

style collapse (â€œgeneric summary voiceâ€)

So: teach it after the model is already obedient.

2) Build summarization as a ladder (recommended datasets)

You want the model to learn two things:

compression skill (keep meaning, drop fluff)

constraint obedience (length, format, focus)

Dataset S1 â€” â€œHard length constraintâ€ summarization

Same input text, different constraints:

â€œSummarize in 10 words.â€

â€œSummarize in 1 sentence.â€

â€œSummarize in 3 bullet points.â€

This forces length control, not just â€œsummarize vaguelyâ€.

Gold rule: The assistant output must contain no new facts not present in input.

Dataset S2 â€” â€œExtractive-firstâ€ summarization (very effective)

Teach a 2-step pattern:

Extract key facts (bullets, short)

Produce a summary only from those facts

Example target format:

Key facts: (3â€“6 bullets)

Summary: (1â€“3 sentences)

This dramatically reduces hallucinations because youâ€™re training an internal â€œgrounding stepâ€.

Later you can remove the â€œKey factsâ€ section once itâ€™s learned.

Dataset S3 â€” â€œSchema summariesâ€

Pick 1â€“2 stable schemas youâ€™ll want later for tool use:

TL;DR: one line

Key points: bullets

Action items: numbered list

Risks: bullets

Open questions: bullets

This is basically â€œstructured output discipline,â€ and it transfers directly into tool-call argument discipline.

Dataset S4 â€” â€œQuery-focused summarizationâ€

This is what makes summarization useful:

Input: long text + user asks:

â€œSummarize only security-relevant parts.â€

â€œSummarize legal implications.â€

â€œSummarize for a manager (non-technical).â€

â€œSummarize with a focus on costs and deadlines.â€

This trains selective attention and reduces generic summaries.

3) Training recipe (simple)

A very practical way to add summarizing without blowing up Phase 3a:

Option A (clean + fast)

Do one dedicated â€œsummarization runâ€ after your minimal Q&A run.

70â€“85% summarization dataset (S1â€“S4 mixed)

15â€“30% replay of your existing â€œformat/tag obedienceâ€ set (3a0â€“3a2)

Option B (integrated)

If you donâ€™t want a separate run:

Add summarization samples as 10â€“20% of your 3a4/3a5 run

Keep the rest task-following + schema

4) What to avoid

Only one style of summary (youâ€™ll get â€œhouse voiceâ€ lock-in)

No constraints (â€œsummarize thisâ€) â†’ model learns vague compression

Training only short summaries â†’ model canâ€™t do â€œdetailed briefâ€

Letting it invent (â€œbackground contextâ€ not in the text)

5) Minimal dataset size

You can get decent summarization behavior surprisingly fast if your data is clean:

100â€“300 high-quality episodes already moves the needle

500â€“1500 gives robustness across styles/constraints

More helps, but quality and constraint diversity matter more than volume here

Ground rule (now explicit)

Random-fed phases (1â€“2): replay = fine

Sequential-fed phases (3a+): replay only with other sequential datasets
â†’ no sharded random text, no concat soup, no leakage of phase-1/2 data

This is correct and important. Youâ€™re training behavior, not language.

How many Phase-3a runs do you actually need?

Not 7.

You need 4 sequential runs, each with a clear behavioral target and limited replay.

âœ… Recommended Phase-3a execution plan (sequential-safe)
Run 1 â€” Protocol & stop discipline

(Phase 3a0 + 3a1)

Primary dataset (â‰ˆ80%)

MyPT tag correctness

One-word / ultra-short answers

Hard stops, no leakage

Replay (â‰ˆ20%)

Earlier protocol samples only

What this locks in

Role obedience

Output termination

Length discipline

ğŸ‘‰ Do not proceed until this is rock-solid.

Run 2 â€” Minimal Q&A obedience

(Phase 3a2)

Primary dataset (â‰ˆ70â€“80%)

Short questions â†’ 1 sentence answers

Length-bounded answers

Unknown â†’ canonical fallback (â€œI donâ€™t know.â€)

Replay (â‰ˆ20â€“30%)

Run-1 sequential data (protocol + short answers)

What this locks in

Natural language instruction following

No verbosity creep

Stable assistant â€œvoiceâ€

Run 3 â€” Task execution + summarization

(Phase 3a4 + summarization)

This is where summarizing belongs.

Primary dataset (â‰ˆ70%)

Multi-step text tasks

Summarization (S1â€“S4 ladder):

length-constrained summaries

extract-then-summarize

schema summaries

query-focused summaries

Replay (â‰ˆ30%)

Run-2 minimal Q&A

Run-1 protocol samples

What this locks in

Compression without hallucination

Step discipline

Selective attention

â€œDo exactly what was asked, nothing moreâ€

Run 4 â€” Structured output & tool-readiness

(Phase 3a5 + 3a6)

Primary dataset (â‰ˆ70â€“80%)

Strict schemas (JSON / tagged blocks)

Fake tool calls (text-only)

Decide: tool vs no tool

Argument filling

Finalization after â€œtool resultâ€

Replay (â‰ˆ20â€“30%)

Summarization + task execution samples

Minimal protocol data (small dose)

What this locks in

Parse-safe outputs

Tool call muscle memory

Zero commentary outside schema

Why summarization is not its own run

Summarization is:

a task

with constraints

and schemas

So it belongs with other task-execution behaviors.
If you isolate it, you risk:

a â€œsummary-voiceâ€ style lock-in

poor transfer to tool usage

Embedding it in Run 3 is the sweet spot.

Sanity checks (important)

After each run, test for:

Regression in earlier behavior (tags, length)

Over-compression (summaries losing key facts)

Schema drift (extra text sneaking in)

If you see drift â†’ increase replay by ~5â€“10%, but only from sequential data.






You donâ€™t need multi-turn for â€œextract key facts â†’ summaryâ€. Do it as a single assistant response with two sections (same turn), so it stays perfectly compatible with your sequential, single-episode pipeline.

Option A (recommended): Single-turn, two-block output

User provides text. Assistant outputs:

KEY_FACTS: bullets (pure extraction; no new info)

SUMMARY: 1â€“3 sentences (must be derivable only from the bullets)

Example gold episode (adapt to your tags):

<myPT_user>
Extract key facts and then summarize. Text:
"ACME reported a data breach on 12 Jan 2026. An exposed API key allowed access to 3,200 customer records. Password hashes were not accessed. ACME rotated keys and notified regulators."
</myPT_user>

<myPT_assistant>
KEY_FACTS:
- ACME reported a data breach on 12 Jan 2026.
- An exposed API key enabled access.
- 3,200 customer records were accessed.
- Password hashes were not accessed.
- Keys were rotated and regulators notified.

SUMMARY:
ACME reported a breach on 12 Jan 2026 caused by an exposed API key, affecting 3,200 customer records. The company rotated keys and notified regulators, and password hashes were not accessed.
</myPT_assistant>


This trains the â€œinternal grounding stepâ€ without tool tags, multi-turn, or unclear formatting.

Option B: Schema version (better for later toolcalls)

Same idea, but in strict JSON (or your preferred schema):

<myPT_assistant>
{
  "key_facts": [
    "â€¦",
    "â€¦"
  ],
  "summary": "â€¦"
}
</myPT_assistant>


This transfers directly into Phase 3b argument discipline.

The three rules that make it work

Key facts must be extractive: only whatâ€™s explicitly stated.

Summary must be supported by key facts: no extra claims.

Hard constraints: number of bullets + max sentences.

Good defaults:

key_facts: 3â€“7 bullets

summary: 1â€“2 sentences (or â€œ<= 25 wordsâ€ for harsh control)

How to vary it (so it generalizes)

For each input text, create 2â€“4 variants:

â€œSummarize in 10 words.â€

â€œSummarize in 1 sentence.â€

â€œSummarize for a manager.â€

â€œSummarize only risks and actions.â€

All still using the same two-block structure.