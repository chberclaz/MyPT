# MyPT Tag Nesting Reference

Canonical reference for all `myPT` special token structures used across SFT phases.
Companion to `core/special_tokens.py` (19 tokens, IDs 50257-50275).

---

## Token Inventory

| Token Pair                 | Count | Generated By | Loss  |
| -------------------------- | ----- | ------------ | ----- |
| `<myPT_system>` / close   | 2     | System       | MASK  |
| `<myPT_user>` / close     | 2     | System       | MASK  |
| `<myPT_assistant>` / close| 2     | Model        | TRAIN |
| `<myPT_user_context>` / close    | 2 | System   | MASK  |
| `<myPT_assistant_context>` / close| 2| System   | MASK  |
| `<myPT_toolcall>` / close | 2     | Model        | TRAIN |
| `<myPT_toolresult>` / close| 2    | System       | MASK  |
| `<myPT_think>` / close    | 2     | Model        | TRAIN |
| `<myPT_cite>` / close     | 2     | Model        | TRAIN |
| `<myPT_eot>`              | 1     | Model        | TRAIN |
| **Total**                  | **19**|              |       |

**TRAIN** = loss computed (model learns to produce these tokens and their content).
**MASK** = loss masked (model sees these but never learns to generate them).

---

## Loss Masking Rules

Everything the **model** generates lives inside `<myPT_assistant>...</myPT_assistant>` blocks.
Everything the **system** injects (system prompt, user text, context, tool results) is loss-masked.

Per-token breakdown:

```
MASK:  <myPT_system>  ...content...  </myPT_system>
MASK:  <myPT_user>  ...content...  </myPT_user>
MASK:  <myPT_user_context>  ...content...  </myPT_user_context>
MASK:  <myPT_assistant_context>  ...content...  </myPT_assistant_context>
MASK:  <myPT_toolresult>  ...content...  </myPT_toolresult>

TRAIN: <myPT_assistant>  ...content...  </myPT_assistant>
TRAIN: <myPT_eot>
```

Inside an assistant block, everything is trained, including nested tags:

```
TRAIN: <myPT_assistant>
TRAIN:   <myPT_think>  ...reasoning...  </myPT_think>
TRAIN:   Answer text with <myPT_cite>source</myPT_cite> inline.
TRAIN:   <myPT_toolcall>  {"name":"..."}  </myPT_toolcall>
TRAIN: </myPT_assistant>
```

**Note:** `<myPT_eot>` is trained so the model learns to emit it as a stop signal after its
final assistant turn. Current `prepare_tool_sft.py` masks eot -- this will be updated
for Phase 1 Format Lock to train on eot.

---

## Nesting Hierarchy

```
conversation
├── <myPT_system> ... </myPT_system>              # once, at start
│
├── TURN (repeat per user-assistant exchange)
│   ├── <myPT_user>
│   │   ├── <myPT_user_context> ... </myPT_user_context>   # optional, RAG chunks
│   │   └── user text
│   ├── </myPT_user>
│   │
│   ├── <myPT_assistant_context> ... </myPT_assistant_context>  # optional, standalone
│   │
│   ├── <myPT_assistant>
│   │   ├── <myPT_think> ... </myPT_think>          # optional, reasoning
│   │   ├── answer text
│   │   │   └── <myPT_cite> ref </myPT_cite>        # optional, inline
│   │   └── <myPT_toolcall> JSON </myPT_toolcall>   # optional, tool request
│   ├── </myPT_assistant>
│   │
│   ├── <myPT_toolresult> ... </myPT_toolresult>    # optional, injected by system
│   │
│   ├── <myPT_assistant>                             # optional, follow-up after tool
│   │   └── final answer (may contain cite)
│   ├── </myPT_assistant>
│   │
│   └── <myPT_eot>                                   # after last assistant in turn
│
└── (next TURN or end)
```

**Key placement rules:**
- `user_context` is **inside** the `<myPT_user>` block, before the user's text
- `assistant_context` is **standalone**, between `</myPT_user>` and `<myPT_assistant>`
- `think` is **inside** the `<myPT_assistant>` block, at the **start** (before answer text)
- `cite` is **inline** within the assistant answer text
- `toolcall` is **inside** the `<myPT_assistant>` block (entire block is just the toolcall)
- `toolresult` is **standalone**, between `</myPT_assistant>` and the next `<myPT_assistant>`
- `eot` follows the **last** `</myPT_assistant>` in a turn

---

## Canonical Examples by SFT Phase

### Phase 1: Format Lock

Teaches the basic `system → user → assistant → eot` skeleton with short responses.

```
<myPT_system>You are MyPT.</myPT_system>
<myPT_user>Say hello.</myPT_user>
<myPT_assistant>Hello.</myPT_assistant>
<myPT_eot>
```

German variant (same structure):

```
<myPT_system>You are MyPT.</myPT_system>
<myPT_user>Sag Hallo.</myPT_user>
<myPT_assistant>Hallo.</myPT_assistant>
<myPT_eot>
```

**Loss mask annotation** (M=mask, T=train):

```
M  <myPT_system>You are MyPT.</myPT_system>
M  <myPT_user>Say hello.</myPT_user>
T  <myPT_assistant>Hello.</myPT_assistant>
T  <myPT_eot>
```

### Phase 2: Operators (Echo / Instruction Following)

Same format as Phase 1 but teaches abstract operator generalization.

```
<myPT_system>You are MyPT.</myPT_system>
<myPT_user>Repeat: OpenAI</myPT_user>
<myPT_assistant>OpenAI</myPT_assistant>
<myPT_eot>
```

Anti-echo contrast (model must NOT blindly copy):

```
<myPT_system>You are MyPT.</myPT_system>
<myPT_user>What does "Blax7" mean?</myPT_user>
<myPT_assistant>Unknown.</myPT_assistant>
<myPT_eot>
```

### Phase 3: Chat SFT (Multi-turn)

Introduces multi-turn conversations. Each turn ends with `<myPT_eot>`.

```
<myPT_system>You are MyPT.</myPT_system>
<myPT_user>What is Python?</myPT_user>
<myPT_assistant>Python is a general-purpose programming language known for its readable syntax.</myPT_assistant>
<myPT_eot>
<myPT_user>What about JavaScript?</myPT_user>
<myPT_assistant>JavaScript is a scripting language primarily used for web development.</myPT_assistant>
<myPT_eot>
```

German multi-turn:

```
<myPT_system>You are MyPT.</myPT_system>
<myPT_user>Was ist Python?</myPT_user>
<myPT_assistant>Python ist eine universelle Programmiersprache, bekannt für ihre lesbare Syntax.</myPT_assistant>
<myPT_eot>
<myPT_user>Und JavaScript?</myPT_user>
<myPT_assistant>JavaScript ist eine Skriptsprache, die hauptsächlich in der Webentwicklung eingesetzt wird.</myPT_assistant>
<myPT_eot>
```

**Loss mask for multi-turn:**

```
M  <myPT_system>You are MyPT.</myPT_system>
M  <myPT_user>What is Python?</myPT_user>
T  <myPT_assistant>Python is a general-purpose programming language...</myPT_assistant>
T  <myPT_eot>
M  <myPT_user>What about JavaScript?</myPT_user>
T  <myPT_assistant>JavaScript is a scripting language...</myPT_assistant>
T  <myPT_eot>
```

### Phase 4: Simple Tool Call

Introduces the `toolcall → toolresult → answer` loop.

```
<myPT_system>You are MyPT. Answer questions using workspace tools when needed.

Tools:
- workspace.search(query, top_k=5) - find relevant documents
- workspace.list_docs() - list all documents
- workspace.get_doc(doc_id or title) - get document text
- workspace.summarize(doc_id or text) - summarize content

Format: <myPT_toolcall>{"name": "workspace.search", "query": "..."}</myPT_toolcall></myPT_system>
<myPT_user>Find documents about machine learning.</myPT_user>
<myPT_assistant><myPT_toolcall>{"name": "workspace.search", "query": "machine learning", "top_k": 3}</myPT_toolcall></myPT_assistant>
<myPT_toolresult>{"documents": [{"doc_id": "ml-101", "title": "Intro to ML", "snippet": "Machine learning is..."}], "total": 1}</myPT_toolresult>
<myPT_assistant>I found 1 document about machine learning: "Intro to ML". It covers the basics of ML.</myPT_assistant>
<myPT_eot>
```

**Loss mask:**

```
M  <myPT_system>...tools description...</myPT_system>
M  <myPT_user>Find documents about machine learning.</myPT_user>
T  <myPT_assistant><myPT_toolcall>{"name": "workspace.search", ...}</myPT_toolcall></myPT_assistant>
M  <myPT_toolresult>{"documents": [...]}</myPT_toolresult>
T  <myPT_assistant>I found 1 document about machine learning...</myPT_assistant>
T  <myPT_eot>
```

### Phase 5: Agentic RAG (Multi-step Tools + Think + Cite)

Full agentic workflow with reasoning, multi-step tool use, and citations.

```
<myPT_system>You are MyPT. Answer questions using workspace tools when needed.

Tools:
- workspace.search(query, top_k=5) - find relevant documents
- workspace.list_docs() - list all documents
- workspace.get_doc(doc_id or title) - get document text
- workspace.summarize(doc_id or text) - summarize content

Format: <myPT_toolcall>{"name": "workspace.search", "query": "..."}</myPT_toolcall></myPT_system>
<myPT_user>What does our security policy say about passwords?</myPT_user>
<myPT_assistant><myPT_think>User wants security policy details about passwords. I should search for it first, then get the full document.</myPT_think><myPT_toolcall>{"name": "workspace.search", "query": "security policy passwords"}</myPT_toolcall></myPT_assistant>
<myPT_toolresult>{"documents": [{"doc_id": "sec-policy-v2", "title": "Security Policy v2", "snippet": "Passwords must be at least 12 characters..."}], "total": 1}</myPT_toolresult>
<myPT_assistant><myPT_toolcall>{"name": "workspace.get_doc", "doc_id": "sec-policy-v2"}</myPT_toolcall></myPT_assistant>
<myPT_toolresult>{"content": "Security Policy v2\n\nPasswords:\n- Minimum 12 characters\n- Must include uppercase, lowercase, numbers\n- Rotate every 90 days\n- No reuse of last 5 passwords"}</myPT_toolresult>
<myPT_assistant>According to the security policy, passwords must:
- Be at least 12 characters long
- Include uppercase, lowercase, and numbers
- Be rotated every 90 days
- Not reuse the last 5 passwords
<myPT_cite>sec-policy-v2</myPT_cite></myPT_assistant>
<myPT_eot>
```

**Loss mask:**

```
M  <myPT_system>...tools...</myPT_system>
M  <myPT_user>What does our security policy say about passwords?</myPT_user>
T  <myPT_assistant><myPT_think>...reasoning...</myPT_think><myPT_toolcall>{"name": "workspace.search", ...}</myPT_toolcall></myPT_assistant>
M  <myPT_toolresult>{"documents": [...]}</myPT_toolresult>
T  <myPT_assistant><myPT_toolcall>{"name": "workspace.get_doc", ...}</myPT_toolcall></myPT_assistant>
M  <myPT_toolresult>{"content": "..."}</myPT_toolresult>
T  <myPT_assistant>According to the security policy...
T  <myPT_cite>sec-policy-v2</myPT_cite></myPT_assistant>
T  <myPT_eot>
```

### Phase 5 Variant: With User Context (Pre-retrieved)

When the RAG pipeline pre-retrieves chunks before the model sees the query:

```
<myPT_system>You are MyPT.</myPT_system>
<myPT_user><myPT_user_context>Document: "Setup Guide" (chunk 2)
To install, run: pip install mypackage
Then configure with: mypackage init --config default</myPT_user_context>
How do I install mypackage?</myPT_user>
<myPT_assistant>To install mypackage, run:
```
pip install mypackage
```
Then initialize it with:
```
mypackage init --config default
```
<myPT_cite>Setup Guide, chunk 2</myPT_cite></myPT_assistant>
<myPT_eot>
```

**Loss mask:**

```
M  <myPT_system>You are MyPT.</myPT_system>
M  <myPT_user><myPT_user_context>...chunks...</myPT_user_context>
M  How do I install mypackage?</myPT_user>
T  <myPT_assistant>To install mypackage...
T  <myPT_cite>Setup Guide, chunk 2</myPT_cite></myPT_assistant>
T  <myPT_eot>
```

### Phase 5 Variant: With Assistant Context (Workspace Metadata)

When the system injects workspace-level metadata for the assistant:

```
<myPT_system>You are MyPT. Answer questions using workspace tools when needed.

Tools:
- workspace.search(query, top_k=5) - find relevant documents
- workspace.list_docs() - list all documents
- workspace.get_doc(doc_id or title) - get document text
- workspace.summarize(doc_id or text) - summarize content

Format: <myPT_toolcall>{"name": "workspace.search", "query": "..."}</myPT_toolcall></myPT_system>
<myPT_user>How many documents do we have?</myPT_user>
<myPT_assistant_context>Workspace: project-alpha | Documents: 47 | Last indexed: 2026-02-08</myPT_assistant_context>
<myPT_assistant>The workspace "project-alpha" contains 47 documents, last indexed on 2026-02-08.</myPT_assistant>
<myPT_eot>
```

**Loss mask:**

```
M  <myPT_system>...tools...</myPT_system>
M  <myPT_user>How many documents do we have?</myPT_user>
M  <myPT_assistant_context>Workspace: project-alpha | Documents: 47 | ...</myPT_assistant_context>
T  <myPT_assistant>The workspace "project-alpha" contains 47 documents...</myPT_assistant>
T  <myPT_eot>
```

---

## JSONL Intermediate Format

All SFT generators output JSONL with this schema. The `prepare_tool_sft.py` script
converts JSONL to tokenized binary with loss masks.

### Basic Episode (Phase 1-3)

```json
{
  "system": "You are MyPT.",
  "messages": [
    {"role": "user", "content": "Say hello."},
    {"role": "assistant", "content": "Hello."}
  ],
  "language": "en"
}
```

### Multi-turn Episode

```json
{
  "system": "You are MyPT.",
  "messages": [
    {"role": "user", "content": "What is Python?"},
    {"role": "assistant", "content": "Python is a programming language."},
    {"role": "user", "content": "And Java?"},
    {"role": "assistant", "content": "Java is an object-oriented language."}
  ],
  "language": "en"
}
```

### Tool Call Episode (Phase 4-5)

```json
{
  "system": "You are MyPT. Answer questions using workspace tools when needed.\n\nTools:\n- workspace.search(query, top_k=5)...",
  "messages": [
    {"role": "user", "content": "Find docs about ML"},
    {"role": "assistant_toolcall", "name": "workspace.search", "arguments": {"query": "ML"}},
    {"role": "toolresult", "name": "workspace.search", "content": {"documents": [...]}},
    {"role": "assistant", "content": "Found ML documentation."}
  ]
}
```

### RAG Context Episode (Phase 5)

```json
{
  "system": "You are MyPT.",
  "messages": [
    {
      "role": "user",
      "content": "How do I install it?",
      "context": "Document: Setup Guide (chunk 2)\nRun: pip install mypackage"
    },
    {"role": "assistant", "content": "Run: pip install mypackage"}
  ]
}
```

### Agentic Episode with Think + Cite (Phase 5)

```json
{
  "system": "You are MyPT. ...",
  "messages": [
    {"role": "user", "content": "What is our password policy?"},
    {
      "role": "assistant_toolcall",
      "name": "workspace.search",
      "arguments": {"query": "password policy"},
      "think": "User wants password policy. Search first, then get full doc."
    },
    {"role": "toolresult", "name": "workspace.search", "content": {"documents": [...]}},
    {"role": "assistant_toolcall", "name": "workspace.get_doc", "arguments": {"doc_id": "sec-v2"}},
    {"role": "toolresult", "name": "workspace.get_doc", "content": {"content": "..."}},
    {
      "role": "assistant",
      "content": "Passwords must be 12+ characters...",
      "cite": "sec-v2"
    }
  ]
}
```

### JSONL Role → Serialized Tag Mapping

| JSONL Role             | Serialized As                                                            |
| ---------------------- | ------------------------------------------------------------------------ |
| `"system": "text"`     | `<myPT_system>text</myPT_system>`                                        |
| `role: "user"`         | `<myPT_user>text</myPT_user>`                                            |
| `role: "user"` + context | `<myPT_user><myPT_user_context>ctx</myPT_user_context>text</myPT_user>` |
| `role: "assistant"`    | `<myPT_assistant>text</myPT_assistant>`                                   |
| `role: "assistant"` + think | `<myPT_assistant><myPT_think>thought</myPT_think>text</myPT_assistant>` |
| `role: "assistant"` + cite | `<myPT_assistant>text <myPT_cite>ref</myPT_cite></myPT_assistant>`     |
| `role: "assistant_toolcall"` | `<myPT_assistant><myPT_toolcall>JSON</myPT_toolcall></myPT_assistant>` |
| `role: "assistant_toolcall"` + think | `<myPT_assistant><myPT_think>thought</myPT_think><myPT_toolcall>JSON</myPT_toolcall></myPT_assistant>` |
| `role: "toolresult"`   | `<myPT_toolresult>JSON</myPT_toolresult>`                                |
| `assistant_context`    | `<myPT_assistant_context>text</myPT_assistant_context>` (standalone)      |
| end of turn            | `<myPT_eot>`                                                              |

---

## Phase Introduction Schedule

| Phase | Tags Introduced                        | Already Known      |
| ----- | -------------------------------------- | ------------------- |
| 1     | system, user, assistant, eot           | --                  |
| 2     | (none new)                             | system, user, assistant, eot |
| 3     | (multi-turn patterns)                  | all Phase 1 tags    |
| 4     | toolcall, toolresult                   | all Phase 1-3 tags  |
| 5     | think, cite, user_context, assistant_context | all Phase 1-4 tags |

This graduated introduction ensures each phase adds minimal new syntax
while the model consolidates previously learned patterns.

---

## Inference Behavior

During inference, the `AgentController` manages the conversation loop:

1. **Format the prompt** using system + user tags (with optional context)
2. **Generate** until `</myPT_assistant>` or `<myPT_eot>` is produced
3. **Parse the output:**
   - If `<myPT_toolcall>` found → extract JSON, execute tool, inject `<myPT_toolresult>`, go to step 2
   - If `<myPT_think>` found → strip from user-visible output, keep for logging
   - If `<myPT_cite>` found → extract reference for verification UI
   - If plain text → return as final answer
4. **Append `<myPT_eot>`** to mark turn complete

---

## Common Mistakes to Avoid

1. **Never nest `<myPT_user>` inside `<myPT_assistant>`** -- roles are strictly separated
2. **Never put `<myPT_toolresult>` inside `<myPT_assistant>`** -- tool results are injected by the system between assistant blocks
3. **Never place `<myPT_eot>` after `<myPT_user>`** -- eot only follows the last `</myPT_assistant>` in a turn
4. **Never put `<myPT_assistant_context>` inside `<myPT_user>`** -- assistant_context is standalone between user and assistant
5. **Never put `<myPT_user_context>` outside `<myPT_user>`** -- user_context is always nested inside the user block
6. **Always place `<myPT_think>` at the start of the assistant block** -- reasoning comes before the answer or toolcall
7. **`<myPT_cite>` is inline** -- it appears within the answer text, not as a separate block
