#!/usr/bin/env python
"""
Clean Code Corpus for Phase 1.5 Induction Training

Processes raw code files (from HuggingFace datasets or local directories) into
clean training text suitable for tokenization. Code is the primary driver of
induction head formation due to massive natural repetition: variable reuse,
function calls, imports, docstrings, and boilerplate patterns.

Pipeline:
1. Filter by file size (remove junk <100 bytes and generated files >100KB)
2. Filter by content quality (min lines, max % comments, no binary/minified)
3. Deduplicate by file hash (exact duplicates)
4. Remove common license headers (reduce boilerplate)
5. Add document boundary markers
6. Write to sharded text files

Usage:
    # Process a directory of code files
    python tools/clean_code_corpus.py \\
        --input_dir data/raw/code_python \\
        --output_dir data/phase1_5_induction_raw/code_python \\
        --language python

    # Process HuggingFace streamed output (JSONL with 'content' field)
    python tools/clean_code_corpus.py \\
        --input_jsonl data/raw/codeparrot_raw.jsonl \\
        --output_dir data/phase1_5_induction_raw/codeparrot \\
        --language python

    # Process with custom shard size
    python tools/clean_code_corpus.py \\
        --input_dir data/raw/code_js \\
        --output_dir data/phase1_5_induction_raw/code_js \\
        --language javascript \\
        --shard_mb 25
"""

import argparse
import hashlib
import json
import os
import re
import sys
from collections import defaultdict
from pathlib import Path
from typing import Iterator, Optional, Tuple

sys.path.insert(0, str(Path(__file__).parent.parent))


# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------

SUPPORTED_EXTENSIONS = {
    "python": [".py"],
    "javascript": [".js", ".jsx", ".mjs"],
    "typescript": [".ts", ".tsx"],
    "java": [".java"],
    "go": [".go"],
    "rust": [".rs"],
    "c": [".c", ".h"],
    "cpp": [".cpp", ".hpp", ".cc", ".hh"],
    "markdown": [".md", ".markdown", ".rst"],
}

# Common license header patterns to strip (first N lines)
LICENSE_PATTERNS = [
    re.compile(r"^#!.*\n", re.MULTILINE),  # Shebang
    re.compile(r"^# -\*-.*-\*-\s*\n", re.MULTILINE),  # Emacs mode line
    re.compile(r"^/\*[\s\S]*?Copyright[\s\S]*?\*/\s*\n?", re.MULTILINE | re.IGNORECASE),
    re.compile(r"^#\s*Copyright.*\n(#.*\n)*", re.MULTILINE | re.IGNORECASE),
    re.compile(r"^//\s*Copyright.*\n(//.*\n)*", re.MULTILINE | re.IGNORECASE),
    re.compile(r"^\"\"\"[\s\S]*?Copyright[\s\S]*?\"\"\"\s*\n?", re.MULTILINE | re.IGNORECASE),
]

# Patterns that indicate generated/minified/binary content
GENERATED_PATTERNS = [
    re.compile(r"^// Code generated .* DO NOT EDIT", re.MULTILINE),
    re.compile(r"^# This file is auto-generated", re.MULTILINE | re.IGNORECASE),
    re.compile(r"^# Generated by", re.MULTILINE | re.IGNORECASE),
    re.compile(r"^\s*\"[A-Za-z0-9+/=]{100,}\"", re.MULTILINE),  # Base64 blobs
]

DOC_START = "==== DOC START ===="
DOC_END = "==== DOC END ===="


# ---------------------------------------------------------------------------
# Filtering
# ---------------------------------------------------------------------------

def is_valid_code_file(content: str, language: str,
                       min_lines: int = 5,
                       max_line_length: int = 1000,
                       max_avg_line_length: int = 200) -> Tuple[bool, str]:
    """
    Check if a code file meets quality criteria.
    
    Returns:
        (is_valid, reason) tuple
    """
    lines = content.split("\n")
    
    # Too few lines
    if len(lines) < min_lines:
        return False, f"too few lines ({len(lines)} < {min_lines})"
    
    # Check for binary/minified content (very long lines)
    long_lines = sum(1 for l in lines if len(l) > max_line_length)
    if long_lines > len(lines) * 0.1:
        return False, f"likely minified ({long_lines} long lines)"
    
    # Check average line length (minified code indicator)
    avg_len = sum(len(l) for l in lines) / max(len(lines), 1)
    if avg_len > max_avg_line_length:
        return False, f"avg line too long ({avg_len:.0f} > {max_avg_line_length})"
    
    # Check for generated content
    for pattern in GENERATED_PATTERNS:
        if pattern.search(content[:2000]):  # Only check first 2KB
            return False, "generated/auto-created file"
    
    # Check for excessive whitespace (likely data files)
    non_empty = sum(1 for l in lines if l.strip())
    if non_empty < len(lines) * 0.3:
        return False, f"too sparse ({non_empty}/{len(lines)} non-empty)"
    
    return True, "ok"


def strip_license_header(content: str, max_header_lines: int = 30) -> str:
    """
    Remove common license headers from the beginning of code files.
    Only removes from the first max_header_lines lines.
    """
    for pattern in LICENSE_PATTERNS:
        # Only match at the very start of the file
        match = pattern.match(content)
        if match and content[:match.end()].count("\n") <= max_header_lines:
            content = content[match.end():]
    
    return content.lstrip("\n")


# ---------------------------------------------------------------------------
# Deduplication
# ---------------------------------------------------------------------------

class ExactDeduplicator:
    """Deduplicate files by SHA-256 hash of content."""
    
    def __init__(self):
        self.seen_hashes = set()
        self.duplicates_found = 0
    
    def is_duplicate(self, content: str) -> bool:
        h = hashlib.sha256(content.encode("utf-8", errors="replace")).hexdigest()
        if h in self.seen_hashes:
            self.duplicates_found += 1
            return True
        self.seen_hashes.add(h)
        return False


# ---------------------------------------------------------------------------
# Input Sources
# ---------------------------------------------------------------------------

def stream_from_directory(input_dir: str, language: str) -> Iterator[Tuple[str, str]]:
    """Stream (filename, content) pairs from a directory of code files."""
    extensions = SUPPORTED_EXTENSIONS.get(language, [f".{language}"])
    input_path = Path(input_dir)
    
    for ext in extensions:
        for filepath in input_path.rglob(f"*{ext}"):
            # Size filter
            size = filepath.stat().st_size
            if size < 100 or size > 100_000:
                continue
            
            try:
                content = filepath.read_text(encoding="utf-8", errors="replace")
                yield str(filepath.relative_to(input_path)), content
            except Exception:
                continue


def stream_from_jsonl(input_jsonl: str, text_field: str = "content") -> Iterator[Tuple[str, str]]:
    """Stream (filename, content) pairs from a JSONL file."""
    with open(input_jsonl, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            line = line.strip()
            if not line:
                continue
            try:
                record = json.loads(line)
                content = record.get(text_field, "")
                filename = record.get("path", record.get("filename", f"doc_{i:08d}"))
                if content and len(content) >= 100 and len(content) <= 100_000:
                    yield filename, content
            except json.JSONDecodeError:
                continue


# ---------------------------------------------------------------------------
# Shard Writer
# ---------------------------------------------------------------------------

class ShardWriter:
    """Writes cleaned documents to sharded text files."""
    
    def __init__(self, output_dir: str, shard_mb: float = 25.0, source_name: str = "code"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.shard_bytes = int(shard_mb * 1024 * 1024)
        self.source_name = source_name
        
        self.shard_idx = 0
        self.current_bytes = 0
        self.current_file = None
        self.total_docs = 0
        self.total_bytes = 0
        self._open_new_shard()
    
    def _open_new_shard(self):
        if self.current_file:
            self.current_file.close()
        path = self.output_dir / f"shard_{self.shard_idx:04d}.txt"
        self.current_file = open(path, "w", encoding="utf-8")
        self.current_bytes = 0
    
    def write_document(self, filename: str, content: str):
        doc = f"{DOC_START}\nSOURCE: {self.source_name} | PATH: {filename}\n====\n{content}\n{DOC_END}\n\n"
        doc_bytes = len(doc.encode("utf-8"))
        
        if self.current_bytes + doc_bytes > self.shard_bytes and self.current_bytes > 0:
            self.shard_idx += 1
            self._open_new_shard()
        
        self.current_file.write(doc)
        self.current_bytes += doc_bytes
        self.total_bytes += doc_bytes
        self.total_docs += 1
    
    def close(self):
        if self.current_file:
            self.current_file.close()
            self.current_file = None
    
    def stats(self) -> dict:
        return {
            "total_docs": self.total_docs,
            "total_shards": self.shard_idx + 1,
            "total_bytes": self.total_bytes,
            "total_mb": self.total_bytes / (1024 * 1024),
        }


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        description="Clean code corpus for Phase 1.5 induction training",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument("--input_dir", type=str,
                             help="Directory containing raw code files")
    input_group.add_argument("--input_jsonl", type=str,
                             help="JSONL file with code documents (one per line)")
    
    parser.add_argument("--output_dir", type=str, required=True,
                        help="Output directory for cleaned shards")
    parser.add_argument("--language", type=str, default="python",
                        choices=list(SUPPORTED_EXTENSIONS.keys()),
                        help="Programming language (for extension filtering)")
    parser.add_argument("--text_field", type=str, default="content",
                        help="JSON field containing code text (for JSONL input)")
    parser.add_argument("--source_name", type=str, default=None,
                        help="Source name for document markers (default: language)")
    parser.add_argument("--shard_mb", type=float, default=25.0,
                        help="Target shard size in MB (default: 25)")
    parser.add_argument("--min_lines", type=int, default=5,
                        help="Minimum lines per file (default: 5)")
    parser.add_argument("--strip_licenses", action="store_true", default=True,
                        help="Strip license headers (default: True)")
    parser.add_argument("--no_strip_licenses", action="store_false", dest="strip_licenses",
                        help="Don't strip license headers")
    parser.add_argument("--max_docs", type=int, default=0,
                        help="Max documents to process (0 = unlimited)")
    parser.add_argument("--report_every", type=int, default=10000,
                        help="Print progress every N documents")
    
    args = parser.parse_args()
    
    source_name = args.source_name or args.language
    
    print("=" * 60)
    print("  Code Corpus Cleaner")
    print("  Phase 1.5 Induction Training Data")
    print("=" * 60)
    print(f"\n  Language:    {args.language}")
    print(f"  Source name: {source_name}")
    print(f"  Shard size:  {args.shard_mb} MB")
    
    if args.input_dir:
        print(f"  Input:       {args.input_dir} (directory)")
        stream = stream_from_directory(args.input_dir, args.language)
    else:
        print(f"  Input:       {args.input_jsonl} (JSONL)")
        stream = stream_from_jsonl(args.input_jsonl, args.text_field)
    
    print(f"  Output:      {args.output_dir}")
    
    # Initialize
    deduper = ExactDeduplicator()
    writer = ShardWriter(args.output_dir, shard_mb=args.shard_mb, source_name=source_name)
    
    stats = defaultdict(int)
    
    print(f"\n  Processing...")
    
    for filename, content in stream:
        stats["total_seen"] += 1
        
        if args.max_docs > 0 and writer.total_docs >= args.max_docs:
            break
        
        # Quality filter
        valid, reason = is_valid_code_file(content, args.language, min_lines=args.min_lines)
        if not valid:
            stats[f"filtered_{reason.split('(')[0].strip()}"] += 1
            continue
        
        # Dedup
        if deduper.is_duplicate(content):
            stats["duplicates"] += 1
            continue
        
        # Strip license headers
        if args.strip_licenses:
            cleaned = strip_license_header(content)
        else:
            cleaned = content
        
        # Final length check after cleaning
        if len(cleaned.strip()) < 50:
            stats["filtered_too_short_after_clean"] += 1
            continue
        
        writer.write_document(filename, cleaned)
        
        if stats["total_seen"] % args.report_every == 0:
            print(f"    Processed {stats['total_seen']:,} files, "
                  f"kept {writer.total_docs:,}, "
                  f"deduped {deduper.duplicates_found:,}")
    
    writer.close()
    
    # Summary
    w_stats = writer.stats()
    print(f"\n" + "=" * 60)
    print(f"  SUMMARY")
    print(f"=" * 60)
    print(f"  Files seen:      {stats['total_seen']:,}")
    print(f"  Files kept:      {w_stats['total_docs']:,}")
    print(f"  Duplicates:      {deduper.duplicates_found:,}")
    print(f"  Filtered:")
    for key, count in sorted(stats.items()):
        if key.startswith("filtered_"):
            reason = key.replace("filtered_", "  ")
            print(f"    {reason}: {count:,}")
    print(f"  Output shards:   {w_stats['total_shards']}")
    print(f"  Output size:     {w_stats['total_mb']:.1f} MB")
    print(f"  Output dir:      {args.output_dir}")
    print()
    
    # Write processing metadata
    meta = {
        "source_name": source_name,
        "language": args.language,
        "input": args.input_dir or args.input_jsonl,
        "total_seen": stats["total_seen"],
        "total_kept": w_stats["total_docs"],
        "duplicates": deduper.duplicates_found,
        "shards": w_stats["total_shards"],
        "total_mb": round(w_stats["total_mb"], 1),
        "filter_stats": {k: v for k, v in stats.items() if k.startswith("filtered_")},
    }
    meta_path = Path(args.output_dir) / "cleaning_metadata.json"
    with open(meta_path, "w") as f:
        json.dump(meta, f, indent=2)
    print(f"  Metadata: {meta_path}")


if __name__ == "__main__":
    main()
